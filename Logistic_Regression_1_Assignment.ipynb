{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0613e21-a796-4290-a2d6-0de77fdbbf68",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b5c86-dd22-4e46-a1e0-c18342fd1a5b",
   "metadata": {},
   "source": [
    "A1\n",
    "\n",
    "**Linear Regression** and **Logistic Regression** are both statistical techniques used for predictive modeling, but they are applied to different types of problems and have distinct characteristics:\n",
    "\n",
    "**Linear Regression:**\n",
    "Linear regression is used for **predicting continuous numerical values**. It models the relationship between a dependent variable (target) and one or more independent variables (predictors) by fitting a linear equation to the observed data. The goal is to find the best-fit line that minimizes the sum of squared differences between the predicted values and the actual values.\n",
    "\n",
    "The equation for simple linear regression with one predictor is typically expressed as:\n",
    "\n",
    "\\[y = \\beta_0 + \\beta_1x\\]\n",
    "\n",
    "Where:\n",
    "- \\(y\\) is the dependent variable (continuous).\n",
    "- \\(x\\) is the independent variable (predictor).\n",
    "- \\(\\beta_0\\) is the intercept.\n",
    "- \\(\\beta_1\\) is the coefficient for the predictor variable.\n",
    "\n",
    "**Example Scenario for Linear Regression:**\n",
    "- **Predicting House Prices**: Linear regression can be used to predict the price of a house based on factors such as square footage, number of bedrooms, and location. In this case, the target variable (house price) is continuous.\n",
    "\n",
    "**Logistic Regression:**\n",
    "Logistic regression is used for **predicting binary or categorical outcomes**. It models the probability of an event occurring as a function of one or more independent variables. The logistic function (sigmoid curve) is used to transform the linear combination of predictors into a probability value between 0 and 1.\n",
    "\n",
    "The equation for logistic regression is:\n",
    "\n",
    "\\[P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}\\]\n",
    "\n",
    "Where:\n",
    "- \\(P(Y=1)\\) is the probability of the binary event occurring.\n",
    "- \\(x\\) is the independent variable (predictor).\n",
    "- \\(\\beta_0\\) is the intercept.\n",
    "- \\(\\beta_1\\) is the coefficient for the predictor variable.\n",
    "\n",
    "Logistic regression is particularly useful when dealing with classification problems, where the target variable is categorical (e.g., yes/no, spam/ham, pass/fail).\n",
    "\n",
    "**Example Scenario for Logistic Regression:**\n",
    "- **Email Spam Classification**: Logistic regression can be used to classify emails as either spam or not spam based on features like the sender, subject, and content of the email. The target variable is binary (spam or not spam).\n",
    "\n",
    "In this scenario, logistic regression is more appropriate than linear regression because it's designed to handle classification tasks with binary outcomes. It models the probability that an email is spam (or not) based on the input features, allowing for clear categorization into one of two classes.\n",
    "\n",
    "In summary, the main difference between linear regression and logistic regression lies in their respective use cases. Linear regression is used for predicting continuous numerical values, while logistic regression is used for predicting binary or categorical outcomes by modeling probabilities. The choice between these two techniques depends on the nature of the target variable in your predictive modeling problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d1e278-a351-4125-9192-94511a3b0738",
   "metadata": {},
   "source": [
    "# Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7dd3d-8d90-4658-a985-950671893402",
   "metadata": {},
   "source": [
    "A2\n",
    "\n",
    "The cost function used in logistic regression is the **binary cross-entropy loss** (also known as log loss). Logistic regression is a supervised learning algorithm used for binary classification problems, where the goal is to predict the probability that an input belongs to one of two classes (e.g., 0 or 1, Yes or No). The binary cross-entropy loss quantifies the dissimilarity between the predicted probabilities and the actual binary labels.\n",
    "\n",
    "Here's the mathematical form of the binary cross-entropy loss for logistic regression:\n",
    "\n",
    "**Cost(y, y_pred) = - [y * log(y_pred) + (1 - y) * log(1 - y_pred)]**\n",
    "\n",
    "Where:\n",
    "- `y` is the true binary label (0 or 1).\n",
    "- `y_pred` is the predicted probability that the input belongs to class 1 (the class labeled as 1).\n",
    "\n",
    "The cost function essentially penalizes the model when its predicted probability `y_pred` diverges from the true label `y`. When `y` is 1 (i.e., the positive class), the cost function penalizes lower predicted probabilities, and when `y` is 0 (i.e., the negative class), it penalizes higher predicted probabilities.\n",
    "\n",
    "The goal in logistic regression is to find the model parameters (coefficients and bias) that minimize this cost function. This is typically done through an optimization algorithm, and one common method is **gradient descent**. Here's a simplified overview of how gradient descent works in logistic regression:\n",
    "\n",
    "1. Initialize the model's parameters (coefficients and bias) with random values or zeros.\n",
    "2. Calculate the gradient of the cost function with respect to these parameters. This gradient represents the direction of the steepest increase in the cost.\n",
    "3. Update the parameters in the opposite direction of the gradient to minimize the cost. This update is controlled by a learning rate, which determines the step size in each iteration.\n",
    "4. Repeat steps 2 and 3 iteratively until convergence, where the cost function reaches a minimum or a predefined number of iterations is reached.\n",
    "\n",
    "The gradient descent algorithm adjusts the model's parameters with each iteration, gradually reducing the cost function, and ultimately finding the parameters that provide the best fit to the data. The learning rate is a hyperparameter that needs to be carefully tuned, as choosing an inappropriate learning rate can lead to slow convergence or overshooting the minimum.\n",
    "\n",
    "In practice, there are also more advanced optimization techniques available, such as stochastic gradient descent (SGD), mini-batch gradient descent, and variants like Adam, which adapt the learning rate during training to improve convergence speed and stability. These methods are often used in combination with logistic regression to optimize the cost function and find the best model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1613c2-ef8b-4397-b2cd-f4fa00d12b4e",
   "metadata": {},
   "source": [
    "# Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884d22c-cf79-4f44-97df-f4c5305dbde5",
   "metadata": {},
   "source": [
    "A3\n",
    "\n",
    "Regularization is a technique used in logistic regression (and other machine learning algorithms) to prevent overfitting, which occurs when a model learns to fit the training data too closely, capturing noise and making it perform poorly on unseen data. Regularization adds a penalty term to the cost function, discouraging the model from assigning excessively large coefficients to the input features. This encourages the model to be simpler and helps it generalize better to new, unseen data.\n",
    "\n",
    "There are two common types of regularization used in logistic regression: **L1 regularization** (Lasso) and **L2 regularization** (Ridge). Each type of regularization adds a different kind of penalty to the cost function:\n",
    "\n",
    "1. **L1 Regularization (Lasso):**\n",
    "   - In L1 regularization, a penalty is applied to the absolute values of the coefficients of the model. The cost function with L1 regularization is modified to include a term proportional to the sum of the absolute values of the coefficients (also known as the L1 norm):\n",
    "   \n",
    "     **Cost(y, y_pred) + λ * Σ|θi|**\n",
    "\n",
    "   - Here, θi represents the coefficients of the model, and λ (lambda) is the regularization parameter that controls the strength of regularization. A higher λ results in stronger regularization.\n",
    "\n",
    "   - L1 regularization encourages sparsity in the model by driving some of the coefficients to become exactly zero. This means that some input features may be effectively ignored by the model, leading to feature selection.\n",
    "\n",
    "   - L1 regularization is useful when you suspect that only a subset of the input features is relevant for making predictions.\n",
    "\n",
    "2. **L2 Regularization (Ridge):**\n",
    "   - In L2 regularization, a penalty is applied to the square of the coefficients of the model. The cost function with L2 regularization is modified to include a term proportional to the sum of the squares of the coefficients (also known as the L2 norm):\n",
    "   \n",
    "     **Cost(y, y_pred) + λ * Σ(θi^2)**\n",
    "\n",
    "   - Like in L1 regularization, λ controls the strength of regularization, but L2 regularization encourages smaller, more evenly distributed coefficients rather than sparsity.\n",
    "\n",
    "   - L2 regularization helps prevent multicollinearity (correlation between input features) by spreading the impact of correlated features across all features.\n",
    "\n",
    "   - L2 regularization is a good choice when you want to prevent overfitting without explicitly selecting a subset of features.\n",
    "\n",
    "By adding either L1 or L2 regularization (or a combination of both) to the logistic regression cost function, you effectively constrain the model's ability to fit the training data too closely. This results in a more stable and generalized model that is less prone to overfitting. The choice between L1 and L2 regularization depends on your specific problem and whether you want to encourage feature selection (L1) or simply prevent overfitting (L2). The regularization strength parameter (λ) should be tuned through techniques like cross-validation to find the optimal balance between model complexity and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b93318-f4c8-4042-b9c7-7cce52d36a89",
   "metadata": {},
   "source": [
    "# Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6489ed5-c7b6-4fb6-ab1d-eb2bdbcc534a",
   "metadata": {},
   "source": [
    "A4\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation used to evaluate the performance of binary classification models, including logistic regression. It helps to visualize and assess how well a model can discriminate between the two classes by varying the classification threshold. The ROC curve is a useful tool for understanding the trade-off between a model's true positive rate (sensitivity) and false positive rate (1-specificity) at different threshold levels.\n",
    "\n",
    "Here's how the ROC curve is created and interpreted:\n",
    "\n",
    "1. **Threshold Variation:** To create an ROC curve, you start by varying the classification threshold used by the logistic regression model. By default, logistic regression predicts a class based on a threshold of 0.5, meaning that if the predicted probability of the positive class is greater than or equal to 0.5, it's classified as the positive class; otherwise, it's classified as the negative class. However, you can change this threshold to get different points on the ROC curve.\n",
    "\n",
    "2. **True Positive Rate (TPR) and False Positive Rate (FPR):** For each threshold value, you calculate two important metrics:\n",
    "   - **True Positive Rate (TPR)**, also known as sensitivity or recall, which is the proportion of true positive predictions (correctly classified positive cases) among all actual positive cases:\n",
    "   \n",
    "     **TPR = TP / (TP + FN)**\n",
    "\n",
    "   - **False Positive Rate (FPR)**, which is the proportion of false positive predictions (incorrectly classified negative cases) among all actual negative cases:\n",
    "   \n",
    "     **FPR = FP / (FP + TN)**\n",
    "\n",
    "   Where:\n",
    "   - TP: True Positives\n",
    "   - FN: False Negatives\n",
    "   - FP: False Positives\n",
    "   - TN: True Negatives\n",
    "\n",
    "3. **ROC Curve Plot:** As you vary the threshold and calculate TPR and FPR for each threshold, you plot these values on an ROC curve. The x-axis represents the FPR, and the y-axis represents the TPR. The ROC curve is a graphical representation of how the model's performance changes across different threshold levels.\n",
    "\n",
    "4. **Area Under the ROC Curve (AUC-ROC):** The area under the ROC curve, denoted as AUC-ROC or simply AUC, is a single numeric metric that summarizes the overall performance of the model. The AUC value ranges from 0 to 1, where a higher value indicates better model performance. An AUC of 0.5 corresponds to a random classifier (no discrimination), and an AUC of 1 represents a perfect classifier.\n",
    "\n",
    "Interpreting the ROC curve and AUC:\n",
    "- A model with an ROC curve closer to the top-left corner (where TPR is high and FPR is low) is considered better at discriminating between classes.\n",
    "- The AUC value quantifies the model's ability to rank positive instances higher than negative instances, regardless of the chosen threshold. A model with an AUC close to 1 is better at distinguishing between classes.\n",
    "\n",
    "In summary, the ROC curve and AUC provide a comprehensive view of a logistic regression model's ability to classify between two classes at various decision thresholds. It's a valuable tool for model evaluation, especially when you need to balance the trade-off between true positives and false positives in your classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b237f-995a-49bf-9f19-4dfbe45ab20d",
   "metadata": {},
   "source": [
    "# Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2538b7-9ce8-4005-97f4-3b4b70b5e466",
   "metadata": {},
   "source": [
    "A5\n",
    "\n",
    "Feature selection is an essential step in building logistic regression models. It involves choosing a subset of the most relevant and informative features (input variables) while discarding irrelevant or redundant ones. Effective feature selection can improve model performance by reducing overfitting, reducing computation time, and improving model interpretability. Here are some common techniques for feature selection in logistic regression:\n",
    "\n",
    "1. **Univariate Feature Selection:**\n",
    "   - Univariate feature selection methods evaluate each feature independently and select the top-ranked features based on some statistical measure, such as chi-squared (for categorical features) or mutual information (for both categorical and continuous features).\n",
    "   - These methods are simple and efficient but may not capture feature interactions.\n",
    "\n",
    "2. **Recursive Feature Elimination (RFE):**\n",
    "   - RFE is an iterative approach that starts with all features and recursively removes the least important feature(s) based on a chosen criterion (e.g., coefficients, p-values) until a desired number of features or a specific performance metric is reached.\n",
    "   - It can be computationally expensive but often yields good results.\n",
    "\n",
    "3. **Feature Importance from Trees:**\n",
    "   - Algorithms like Random Forest and Gradient Boosting can provide feature importance scores based on how much each feature contributes to the tree-based models' performance.\n",
    "   - You can select the top-ranked features based on their importance scores.\n",
    "\n",
    "4. **L1 Regularization (Lasso):**\n",
    "   - As mentioned earlier, L1 regularization in logistic regression encourages sparsity by driving some coefficients to zero. Features with non-zero coefficients are considered the most important and are selected for the model.\n",
    "   - L1 regularization can simultaneously perform feature selection and model training.\n",
    "\n",
    "5. **Correlation-Based Feature Selection:**\n",
    "   - Correlation analysis helps identify features that are highly correlated with the target variable. Features with a high correlation are often considered more important for prediction.\n",
    "   - You can select features with the highest absolute correlation coefficients with the target variable.\n",
    "\n",
    "6. **Sequential Feature Selection:**\n",
    "   - Sequential feature selection methods, such as forward selection and backward elimination, iteratively add or remove features based on a performance metric (e.g., AIC, BIC, or cross-validation scores) until an optimal subset of features is found.\n",
    "   - These methods can be more exhaustive but ensure that the selected feature subset is optimized for the chosen metric.\n",
    "\n",
    "7. **Principal Component Analysis (PCA):**\n",
    "   - PCA is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated features (principal components).\n",
    "   - You can select a subset of these principal components that capture most of the variance in the data.\n",
    "\n",
    "8. **Feature Engineering and Domain Knowledge:**\n",
    "   - Sometimes, domain knowledge can guide feature selection by helping you identify which features are likely to be the most informative for the specific problem.\n",
    "   - Feature engineering, which involves creating new features or transformations of existing ones, can also improve the feature set.\n",
    "\n",
    "The choice of feature selection technique depends on the dataset, the problem, and the goals of the analysis. It's often a good practice to combine multiple methods or experiment with different feature subsets to find the best combination that improves model performance. Careful feature selection can lead to more interpretable, efficient, and accurate logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38597648-e67b-4328-8bd4-7136f8f73339",
   "metadata": {},
   "source": [
    "# Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa720780-7959-4f7b-8b8c-601f10431743",
   "metadata": {},
   "source": [
    "A6\n",
    "\n",
    "Handling imbalanced datasets in logistic regression is important because traditional logistic regression models can be biased towards the majority class when one class significantly outnumbers the other. This can lead to poor performance, especially when the minority class is of greater interest. Here are some strategies for dealing with class imbalance in logistic regression:\n",
    "\n",
    "1. **Resampling Techniques:**\n",
    "   - **Oversampling the Minority Class:** You can increase the number of instances in the minority class by randomly duplicating samples or generating synthetic samples (e.g., using SMOTE or ADASYN) to balance the class distribution.\n",
    "   - **Undersampling the Majority Class:** You can reduce the number of instances in the majority class by randomly removing samples to balance the class distribution.\n",
    "   - **Combining Over- and Under-Sampling:** A combination of oversampling and undersampling techniques can be used to balance the dataset effectively.\n",
    "\n",
    "2. **Changing the Classification Threshold:**\n",
    "   - By default, logistic regression uses a threshold of 0.5 to make binary classifications. You can adjust this threshold to achieve the desired balance between precision and recall. Decreasing the threshold (e.g., to 0.4) can increase sensitivity (true positive rate) at the expense of specificity (true negative rate), making the model more sensitive to the minority class.\n",
    "\n",
    "3. **Weighted Logistic Regression:**\n",
    "   - Many implementations of logistic regression allow you to assign different weights to the classes. You can assign higher weights to the minority class to make the algorithm pay more attention to it during training.\n",
    "\n",
    "4. **Cost-Sensitive Learning:**\n",
    "   - In cost-sensitive learning, you assign different misclassification costs to the different classes. By assigning a higher cost to misclassifying the minority class, you can encourage the model to focus on improving its performance for that class.\n",
    "\n",
    "5. **Use of Different Algorithms:**\n",
    "   - Sometimes, logistic regression might not be the best choice for imbalanced datasets. Consider using other algorithms that are naturally more robust to class imbalance, such as decision trees, random forests, gradient boosting, or support vector machines.\n",
    "\n",
    "6. **Ensemble Methods:**\n",
    "   - Ensemble methods like Random Forest and Gradient Boosting often handle class imbalance well. They combine multiple base models and can balance the class distribution more effectively.\n",
    "\n",
    "7. **Anomaly Detection Techniques:**\n",
    "   - If the minority class represents anomalies or rare events, consider treating the problem as an anomaly detection task instead of a traditional binary classification problem. Anomaly detection algorithms are designed to detect rare events.\n",
    "\n",
    "8. **Collect More Data:**\n",
    "   - If feasible, collect more data for the minority class to balance the dataset naturally. This is often the most effective but not always the most practical solution.\n",
    "\n",
    "9. **Evaluation Metrics:**\n",
    "   - When evaluating the model's performance, avoid relying solely on accuracy. Instead, consider metrics such as precision, recall, F1-score, and the area under the ROC curve (AUC-ROC) that provide a more comprehensive view of model performance, especially in imbalanced scenarios.\n",
    "\n",
    "The choice of strategy depends on the specific dataset and problem at hand. It's common to experiment with multiple techniques to determine which one or combination of methods yields the best results. Additionally, domain knowledge and the relative importance of each class should also guide your approach to handling imbalanced datasets in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ce926-c30d-489c-8cea-a6d5cfc6e16a",
   "metadata": {},
   "source": [
    "# Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3690d-aa83-45d8-ac85-c9f787f6db83",
   "metadata": {},
   "source": [
    "A7\n",
    "\n",
    "Certainly, logistic regression, like any other modeling technique, has its own set of challenges and issues that can arise during implementation. Here are some common issues and potential solutions when working with logistic regression:\n",
    "\n",
    "1. **Multicollinearity:**\n",
    "   - **Issue:** Multicollinearity occurs when two or more independent variables in the model are highly correlated, making it difficult to distinguish their individual effects on the dependent variable.\n",
    "   - **Solution:** \n",
    "     - Detect multicollinearity by examining correlation matrices or variance inflation factors (VIFs).\n",
    "     - Address multicollinearity by removing one of the correlated variables, combining them into a single variable, or applying dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
    "     - Prioritize domain knowledge to decide which variable to retain or merge.\n",
    "\n",
    "2. **Imbalanced Data:**\n",
    "   - **Issue:** When dealing with imbalanced classes, logistic regression may have difficulty correctly classifying the minority class.\n",
    "   - **Solution:** Refer to the strategies mentioned in the previous response for handling imbalanced datasets, such as resampling, adjusting classification thresholds, or using different algorithms.\n",
    "\n",
    "3. **Overfitting:**\n",
    "   - **Issue:** Logistic regression can overfit the training data, resulting in poor generalization to unseen data.\n",
    "   - **Solution:** \n",
    "     - Use regularization techniques (e.g., L1 or L2 regularization) to penalize large coefficients and prevent overfitting.\n",
    "     - Cross-validation helps estimate the model's performance on unseen data and can aid in selecting the appropriate level of complexity.\n",
    "\n",
    "4. **Underfitting:**\n",
    "   - **Issue:** Logistic regression may underfit when the model is too simple to capture the underlying relationships in the data.\n",
    "   - **Solution:** \n",
    "     - Consider feature engineering to create more informative features.\n",
    "     - Use more complex models, such as decision trees or ensemble methods, when logistic regression is not sufficient.\n",
    "\n",
    "5. **Missing Data:**\n",
    "   - **Issue:** Logistic regression cannot handle missing data directly.\n",
    "   - **Solution:** \n",
    "     - Impute missing data using techniques like mean imputation, median imputation, or sophisticated methods such as multiple imputation.\n",
    "     - Carefully assess the impact of imputation methods on the model's performance and bias.\n",
    "\n",
    "6. **Outliers:**\n",
    "   - **Issue:** Outliers can have a disproportionate impact on logistic regression coefficients.\n",
    "   - **Solution:** \n",
    "     - Identify and handle outliers by using robust techniques or transformations (e.g., Winsorization) that downweight extreme values.\n",
    "     - Consider removing or winsorizing outliers if they are artifacts or data entry errors.\n",
    "\n",
    "7. **Model Interpretability:**\n",
    "   - **Issue:** Logistic regression provides coefficient estimates, but interpreting them can be challenging.\n",
    "   - **Solution:** \n",
    "     - Examine coefficients to understand the direction and magnitude of the effects of independent variables on the log-odds of the outcome.\n",
    "     - Utilize odds ratios to describe the change in odds associated with a one-unit change in an independent variable.\n",
    "\n",
    "8. **Non-Linear Relationships:**\n",
    "   - **Issue:** Logistic regression assumes a linear relationship between independent variables and the log-odds of the outcome.\n",
    "   - **Solution:** \n",
    "     - Transform variables to make them more linear (e.g., log transformations).\n",
    "     - Consider using polynomial features to capture non-linear relationships.\n",
    "     - If non-linearity is significant, explore non-linear models like decision trees or neural networks.\n",
    "\n",
    "9. **Feature Selection:**\n",
    "   - **Issue:** Selecting the right set of features is crucial, and choosing too many or too few can impact model performance.\n",
    "   - **Solution:** \n",
    "     - Use feature selection techniques such as univariate selection, recursive feature elimination, or feature importance from trees to identify the most informative features.\n",
    "     - Consider domain knowledge when selecting features.\n",
    "\n",
    "Addressing these issues requires a combination of data preprocessing, feature engineering, model selection, and careful evaluation. The choice of strategy will depend on the specific challenges posed by the dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3d1d1-dd15-4946-a8a1-3cd2655253bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
